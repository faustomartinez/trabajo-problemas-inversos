\documentclass[12pt]{beamer}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{float}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newtheorem{proposition}{Proposición}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning,fit}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[spanish]{babel}
\newtheorem*{defs}{Definición}
\usepackage{xcolor}

\title{Métodos iterativos para la solución de problemas inversos discretos}

\author{Jazmín Rivas, Kevin Luna, Fausto Martinez, Aaron Klarreich}
\date{Martes 17 de diciembre}

\begin{document}
	
	\begin{frame}
		\titlepage
	\end{frame}
	
	\section{Introducción} 
	% En esta sección va a ir Motivación, Métodos iterativos (definiciones, formula, Jacobi/G-S, etc).
	
	\begin{frame}{Motivación}
		
		\ En esta presentación vamos a estudiar el problema de desenfoque de imágenes con un ejemplo concreto.
		
		\begin{itemize}
			\item Pasar de una imagen a un sistema lineal $Ax = b$
			\item Métodos iterativos
			\item Métodos iterativos de Krylov
			\item Métodos de regularización
		\end{itemize}
		
	\end{frame}
	
	\begin{frame}{Ejemplo}
		
		\ A lo largo de la presentación vamos a trabajar con el siguiente ejemplo:
		\begin{figure}[htp]
			\centering
			\includegraphics[width=3.5cm]{Imagenes/Imagenes/x_true.png}
			\caption{Un mate}
		\end{figure}
		
		\ Con los conocimientos que vayamos adquiriendo trataremos de reconstruirlo de la manera más óptima posible.
		
	\end{frame}
	
	\begin{frame}{Preliminares}
		
		\begin{itemize}
			\item Dada una imagen en blanco y negro de $n \times n$ pixels, podemos representarla como una matriz de $n \times n$ entradas de $1$ y $0$ que eventualmente será vectorizada.
			\item Recordemos que un operador lineal se puede representar como una matriz $\mathbf A$.
			\item Vamos a reconstruir la imagen $\mathbf x$ a partir de un operador de desenfoque $\mathbf A$ y una imagen ya con ruido $\mathbf b = b_{true}+e$, resolviendo el sistema $\mathbf Ax=b$.
		\end{itemize}
		
	\end{frame}
	
	\begin{frame}{Operador de Desenfoque}
		
		Primero veamos cómo la operador de desenfoque $\mathbf A$ afecta a un punto:
		
		\begin{figure}[htp]
			\centering
			\includegraphics[width=3cm]{Imagenes/Imagenes/y_pixel.png}
			\qquad\tikz[baseline=-\baselineskip]\draw[ultra thick,->] (0,0) -- ++ (1,0);\qquad % NO PUDE ENCONTRAR COMO CENTRARLA
			\includegraphics[width=3cm]{Imagenes/Imagenes/Ay_pixel.png}
			\label{fig:mate}
		\end{figure}
		
		
	\end{frame}
	
	\begin{frame}{Métodos Iterativos}
		
		\ Ante las limitaciones que traen los métodos de resolución de problemas del estilo $Ax = b$, los métodos iterativos buscan superar los problemas de estos métodos para producir soluciones a estos sistemas. \\
		\ Si consideramos $A = M-N$, tenemos que: $$(M-N)x = b$$ o bien: $$x = M^{-1}(b+Nx)$$ lo que nos da la fórmula recursiva $$x_{n+1} = M^{-1}(b+Nx_n)$$
		
	\end{frame}
	
	\begin{frame}{Métodos Estándar}
		
		\ Partiendo de la forma $x_{n+1} = M^{-1}(b+Nx_n)$, si tomamos $N = M-A$ tenemos que: $$x_{n+1}=M^{-1}(b+(M-A)x_{n})=M^{-1}b+x_n-M^{-1}Ax_n$$ luego, $$x_{n+1}=x_n+M^{-1}(b-Ax_n)$$ La cual es la fórmula general para cualquier método iterativo. Más aún, si descomponemos a $A$ como $A = L+D+U$ como sus partes \textit{lower}, \textit{diagonal} y \textit{upper} podemos definir los métodos iterativos más comunes.
		
	\end{frame}
	
	\begin{frame}{Método de Jacobi}
		
		\ Como $A = L+D+U$, tenemos que $$Ax=b \iff Dx=-(L+U)x+b$$ con lo que podemos escribir la forma recursiva como: $$Dx_{n+1}=-(L+U)x_n+b$$ y esto nos dice que: $$x_{n+1}=-D^{-1}(L+U)x_n+D^{-1}b$$ la cual es la iteración del método de Jacobi.
		
	\end{frame}
	
	\begin{frame}{Método de Gauss-Seidel}
		
		\ De la misma forma, podemos decir que $$Ax=b \iff (L+D)x=-Ux+b$$ y nos induce la forma recurisva $$(L+D)x_{n+1}=-Ux_n+b$$ que podemos reescribir como la iteración del método de Gauss-Seidel: $$x_{n+1}=-(L+D)^{-1}Ux_n+(L+D)^{-1}b$$
	\end{frame}
	
	\begin{frame}
		
		\textcolor{blue}{\Large Veamos que resulta de intentar resolver el problema con Jacobi o Gauss-Seidel}
		
	\end{frame}
	
	\section{Teoría}
	% Acá va a ir todo lo de Krylov, métodos de proyección, Arnoldi, GMRES, LSQR, Tikhonov, etc. 
	
	\begin{frame}{Métodos de Krylov}
		
		\begin{defs}
			Dada $A$ una matriz y $v, Av, \dots , A^{m-1}v \in \mathbb{R}^d$ vectores linealmente independientes, llamamos subespacio de Krylov de dimensión $m$ a: $$\mathcal{K}_{m}(A, v) = \langle v, Av, \ldots, A^{m-1}v \rangle$$
		\end{defs} 
		Los métodos de Krylov son métodos iterativos que utilizan subespacios de Krylov que, dado $x_0$ una aproximación inicial de la solución del sistema, se define el subespacio $\mathcal{K}_{m}(A, r_0)$, con $r_0=b-Ax_0$.
		
	\end{frame}
	
	\begin{frame}{Motivación del Método}
		
		\ Sea $x_{true}=A^{-1}b$ y $x_0$ una aproximación inicial para el método, resolver el sistema original es equivalente a resolver $Az=r_0$ si $z=x_{true}-x_0$. \\ \ Sea $p(x)$ el polinomio de menor grado $i$ tal que $p(A)=0$. Por el teorema de Cayley-Hamilton, tenemos que $i\leq d$. Entonces tenemos que: $$p(A)=a_0+Aa_1+\dots+a_{i-1}A^{i-1}+a_iA^i=0$$ 
		
	\end{frame}
	
	\begin{frame}{Motivación del Método}
		
		Por lo que vale que $$A^{-1}a_0=-(a_1I+a_2A+\dots+a_{i-1}A^{i-2}+a_iA^{i-1})$$ Y si multiplicamos por $r_0$ tenemos que: $$z=A^{-1}r_0=-\frac{1}{a_0}(a_1I+a_2A+\dots+a_{i-1}A^{i-2}+a_iA^{i-1})r_0$$ Por lo que $$z\in \langle r_0,Ar_0,\dots,A^{i-1}r_0 \rangle = \mathcal{K}_{m}(A, r_0)$$ o bien, $$x_{true} \in x_0 + \mathcal{K}_{m}(A, r_0).$$
		
	\end{frame}
	
	\begin{frame}{Métodos de Proyección}
		
		\ Como $x_{true} \in x_0 + \mathcal{K}_{m}(A, r_0)$, naturalmente la solución $x_n$ puede estar dada por $$x_n = x_0 + y_n, \quad  y_n \in \mathcal{K}_n(A, r_0).$$ Dado $x_0 \in \mathbb{R}^d$ una aproximación inicial a nuesta solución y $r_0 := b - Ax_0$, definimos al residuo en la iteración $n$ está dado por: $$r_n := b - Ax_n = b - A(x_0 + y_n) = r_0 - Ay_n, \quad r_n\in \ \mathcal{K}_{n+1}(A, r_0).$$ Veamos algunos enfoques para determinar $x_n$ de manera óptima.
		
	\end{frame}
	
	\begin{frame}{Enfoque de Petrov-Galerkin}
		
		\begin{flushleft}
			\textbf{Enfoque de Petrov-Galerkin}
		\end{flushleft}
		\ Aquí vamos a pedir que el residuo sea ortogonal a un subespacio $\mathcal{L} \subset \mathbb{C}^{n}$ de dimensión $n$, es decir, 	$$r_n \perp \mathcal{L}_n, \quad dim(\mathcal{L}_n) = n$$
		
		
		\ Por lo general consideramos $\mathcal{L}_n = \mathcal{K}_{n}(A, r_0)$.
		
	\end{frame}
	
	\begin{frame}{Método de Arnoldi}
		
		\ Sea $V_i$ la matriz de tamaño $n\times i$ con $v_1, ..., v_i$ como columnas, $H_i$ la matriz de Hessenberg de tamaño $(i+1)\times i$ cuyos elementos no nulos son los $h_{ij}$ descritos en el algoritmo de Arnoldi, entonces se verifica que: $$AV_j=V_{j+1}H_i$$
		
		
		
	\end{frame}
	
	\begin{frame}
		
		\begin{algorithm}[H]
			\caption{Algoritmo de Arnoldi}\label{algorithm_1}
			\begin{algorithmic}[1]
				\State Comencemos con $v_1=\frac{v}{\norm{v}}$
				\For{$j \in \{1, 2, \dots, i\}$}
				
				\State Computamos $h_{ij} = v_i^TAv_j$ para $i \in \{1, 2, \dots, j\}$
				\State $w_j = Av_j - \sum_{i=1}^j h_{ij} v_i$
				\State $h_{j+1,j} = \|w_j\|_2$
				\If{$h_{j+1,j} = 0$}
				
				\State Stop
				\EndIf
				\EndFor
				\State $v_{j+1} = w_j / h_{j+1,j}$
			\end{algorithmic}
		\end{algorithm}   
		
	\end{frame}
	
	\begin{frame}{GMRES}
		
		\ GMRES (Generalized Minimal Residual Method) busca minimizar $J(y)=||b-Ax||_2$ a través de sus componentes ortonormales, minimizando en cada paso los vectores residuales. \\
		\ Sea $x_0$ una aproximación inicial, tenemos que para todo $x_i\in\mathcal{K}_m+x_0$ vale que $$x=x_0+V_my$$ donde $V_m$ es la misma matriz definida en el método de Arnoldi, e $y$ es un vector de tamaño $m$.
		
	\end{frame}
	
	\begin{frame}{GMRES}
		
		Sean $\beta = ||r_0||_2$ y $v_1=\frac{r_0}{||r_0||_2}$, y teniendo en cuenta la igualdad en el metodo de Arnoldi, podemos reescribir el problema a minimizar como:
		\begin{equation*}
			\begin{split}
				b-Ax&=b-A(x_0+V_my)\\
				`    &= r_0-AV_my\\
				&= \beta v_1-V_{m+1}H_my\\
				&= V_{m+1}(\beta e_1 - H_my)
			\end{split}
		\end{equation*}
		luego, 
		\begin{equation*}
			\begin{split}
				J(y)&=||b-Ax_i||_2\\
				&=||V_{m+1}(\beta e_1 - H_my_m)||_2    
			\end{split}
		\end{equation*}
		y como los vectores columna de $V_{m+1}$ son ortonormales, entonces:
		$$J(y)=||\beta e_1 - H_my||_2$$
		
	\end{frame}
	
	\begin{frame}
		
		\begin{algorithm}[H]
			\caption{Algoritmo GMRES}
			\begin{algorithmic}[1]
				\State \textbf{Dada} $\mathbf{A} \in \mathbb{R}^{n \times n}$, $\mathbf{b} \in \mathbb{R}^n$, $\mathbf{x}^{(0)}$, calcular $\mathbf{r}^{(0)} = \mathbf{b} - \mathbf{A} \mathbf{x}^{(0)}$
				\State $\mathbf{v}_1 = \frac{\mathbf{r}^{(0)}}{\|\mathbf{r}^{(0)}\|}$
				\For{$k = 1, 2, \dots, m$}
				\State Calcular $\mathbf{w_k} = \mathbf{A} \mathbf{v}_k$
				\For{$j = 1, \dots, k$}
				\State $h_{jk} = \mathbf{w}^T \mathbf{v}_j$
				\State $\mathbf{w_k} := \mathbf{w_k} - h_{jk} \mathbf{v}_j$
				\EndFor
				\State $h_{k+1,k} = \|\mathbf{w_k}\|$
				\If{$h_{k+1,k} \neq 0$}
				\State $\mathbf{v}_{k+1} = \frac{\mathbf{w_k}}{h_{k+1,k}}$
				\EndIf
				\EndFor
				\State Resolver el problema de mínimos cuadrados $\min_y \|\beta \mathbf{e}_1 - \mathbf{H}_k y\|_2$
				\State Actualizar la solución: $\mathbf{x}^{(k)} = \mathbf{x}^{(0)} + \mathbf{V}_k y$
				\If{$\|\mathbf{r}^{(k)}\| <$ tolerancia}
				\State Romper el ciclo
				\EndIf
			\end{algorithmic}
		\end{algorithm}
		
	\end{frame}
	
	\begin{frame}
		
		\textcolor{blue}{\Large Veamos como podemos aplicar GMRES para nuestro problema.}
		
	\end{frame}
	
	\begin{frame}{Descomposición SVD}
		
		\ Sea $A \in \mathbb{R}^{m \times n}$, la descomposición SVD nos permite escrbrir a $A$ como: $$A = U \Sigma V^T = \sum_{i=1}^{n} \sigma_i u_i v_i^T$$
		
	\end{frame}
	
	
	\begin{frame}{Regularización de Tikhonov}
		
		\ La regularización es una técnica que busca soluciones de sistemas lineales mal condicionados agregando un término regularizante. Tikhonov en particular busca resolver $$\min_{x} \left( \| \mathbf{A}x - \mathbf{b} \|_2^2 + \lambda^2 \| \mathbf{x} \|_2^2 \right)$$ con $\lambda > 0$ el parámetro regularizante. Y la solución cumple que: \[ 
		\boxed{(A^TA + \lambda^2 \mathrm{Id})x = A^Tb}
		\]
		
	\end{frame}
	
	\begin{frame}{Regularización de Tikhonov generalizada}
		\ En general, se suele pedir que la solución minimice cierta cantidad 
		\[
		\Omega(x) = \| L(x) \|_2
		\] con $L$ un operador que bien puede ser la identidad o el operador de derivación. Si además conocemos un estimador inicial de la solución $x^*$, $$\Omega(x) = \| L(x - x^*) \|^2.$$ Y en definitiva queremos encontrar $$\min \left( \| \mathbf{A}x - \mathbf{b} \|_2^2 + \lambda^2 \| \mathbf{L(x - x^*)} \|_2^2 \right)$$
		
		
	\end{frame}
	
	\begin{frame}{Curva $\mathcal{L}$}
		
		\ Una forma de hallar el valor regularizador es la Curva L, que consiste en graficar la relación entre la norma del residuo y la de la solución: $$\mathcal{L}(\lambda) := \{(\phi (\| \mathbf{A}x_\lambda - \mathbf{b} \|^2),\phi (\| x_\lambda  \|^2) ): \lambda > 0\}$$ con $\phi$ una función monótona creciente, generalmente $\phi(t) = \log(t)$ o $\phi(t) = t$.
		
	\end{frame}
	
	\begin{frame}
		
		\textcolor{blue}{\Large Veamos si ambos métodos de Tikhonov mejoran nuestra solución.}
		
	\end{frame}
	
	
	
	
	
	\begin{frame}{LSQR y LSMR}
		
		\ Si consideramos el subespacio de Krylov $\mathcal{K}_m (AA^t, r_0)$ podemos defirnir los métodos LSQR y LSMR.
		\begin{itemize}
			\item LSQR: minimiza el problema de mínimos cuadrados $||Ax-b||$
			\item LSMR: resuelve el problema regularizado $||(A^tA+\lambda ^2 I)x-A^tb||$
		\end{itemize}
		
	\end{frame}
	
	\begin{frame}
		
		\textcolor{blue}{\Large Pasemos a ver qué resulta de usar estos métodos en nuestro problema.}
		
	\end{frame}
	
	\begin{frame}
		
		\textcolor{blue}{\huge Conclusiones}
		
	\end{frame}
	
\end{document}
